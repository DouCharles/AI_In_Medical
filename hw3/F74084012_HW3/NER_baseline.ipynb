{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "運行環境:Colab<br>\n",
        "最後一段code為失敗的CRF (BiLSTM)<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "xAUewAJ0RrnV"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6LWkA3P8HZO",
        "outputId": "d72e9816-8851-48ae-fdd3-212be7faf7ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Load your cloud drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "folder = 'drive/MyDrive/forth_up/AI_IN_medical/hw3'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTWqf_Q3myFA",
        "outputId": "45584a49-aebb-475d-9fcf-115f5dd12ed2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "drive  sample_data\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7FSX3WclJ-F"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CloK6THIRrnV"
      },
      "source": [
        "## Preprocessing\n",
        "* Change input data (ex. train.txt) into CRF model input format (ex. train.data)\n",
        "    * CRF model input format (ex. train.data):\n",
        "        ```\n",
        "        肝 O\n",
        "        功 O\n",
        "        能 O\n",
        "        6 B-med_exam\n",
        "        8 I-med_exam\n",
        "        ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnW-IP6iRXF_"
      },
      "source": [
        "## Data pre-processing\n",
        "- annot = article_id, start_pos, end_pos, entity_text, entity_type (columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "5avpn8FTRrnV"
      },
      "outputs": [],
      "source": [
        "file_path = f'{folder}/sample_data.txt'\n",
        "\n",
        "with open(file_path, 'r', encoding='utf8') as f:\n",
        "    file_text = f.read().encode('utf-8').decode('utf-8-sig')\n",
        "\n",
        "datas = file_text.split('\\n\\n--------------------\\n\\n')[:-1]\n",
        "\n",
        "with open(f\"{folder}/processed_data.txt\", \"w\") as f:\n",
        "    for article_id, data in enumerate(datas):\n",
        "        data=data.split('\\n')\n",
        "        content=data[0]\n",
        "\n",
        "        annotations=data[1:]\n",
        "        row = list()\n",
        "        for annot in annotations[1:]:\n",
        "            annot=annot.split('\\t') #annot= article_id, start_pos, end_pos, entity_text, entity_type\n",
        "            row.append(annot)\n",
        "\n",
        "        df = pd.DataFrame(row, columns=data[1].split('\\t'))\n",
        "        position_cols = ['start_position', 'end_position']\n",
        "        df[position_cols] = df[position_cols].astype('int')\n",
        "\n",
        "        tmp_label_list = np.array(['O'] * len(content), dtype=object)\n",
        "        for i in range(len(df)):\n",
        "            start, end, etype = df['start_position'][i], df['end_position'][i], df['entity_type'][i]\n",
        "            # print(start, end, etype)\n",
        "            tmp_label_list[start] = \"B-\" + str(etype)\n",
        "            tmp_label_list[start+1:end] = \"I-\" + str(etype)\n",
        "\n",
        "        for i, row in enumerate(zip(list(content), tmp_label_list)):\n",
        "            f.write(\" \".join(row) + '\\n')\n",
        "        \n",
        "        f.write('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuYBIWnlRrnW"
      },
      "source": [
        "## NER model\n",
        "### CRF (Conditional Random Field model)\n",
        "* Using `sklearn-crfsuite` API\n",
        "\n",
        "    (you may try `CRF++`, `python-crfsuite`, `pytorch-crfsuite`(neural network version))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mI9UJRfKRrnW",
        "outputId": "bfbbecb8-d14d-4c39-f740-fb5898eb4294"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sklearn-crfsuite in /usr/local/lib/python3.8/dist-packages (0.3.6)\n",
            "Requirement already satisfied: python-crfsuite>=0.8.3 in /usr/local/lib/python3.8/dist-packages (from sklearn-crfsuite) (0.9.8)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.8/dist-packages (from sklearn-crfsuite) (0.8.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from sklearn-crfsuite) (1.15.0)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.8/dist-packages (from sklearn-crfsuite) (4.64.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install sklearn-crfsuite\n",
        "import sklearn_crfsuite\n",
        "\n",
        "from sklearn_crfsuite import scorers\n",
        "from sklearn_crfsuite import metrics\n",
        "from sklearn_crfsuite.metrics import flat_classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "6GMS2hCoRrnW"
      },
      "outputs": [],
      "source": [
        "def CRF(x_train, y_train, x_test, y_test):\n",
        "    # Doc: https://sklearn-crfsuite.readthedocs.io/en/latest/api.html#module-sklearn_crfsuite\n",
        "    crf = sklearn_crfsuite.CRF(\n",
        "        algorithm='lbfgs',\n",
        "        c1=0.1,\n",
        "        c2=0.1,\n",
        "        max_iterations=100,\n",
        "        all_possible_transitions=True,\n",
        "    )\n",
        "    crf.fit(x_train, y_train)\n",
        "\n",
        "    y_pred = crf.predict(x_test)\n",
        "    y_pred_mar = crf.predict_marginals(x_test)\n",
        "\n",
        "    labels = list(crf.classes_)\n",
        "    labels.remove('O')\n",
        "    f1score = metrics.flat_f1_score(y_test, y_pred, average='weighted', labels=labels)\n",
        "    sorted_labels = sorted(labels,key=lambda name: (name[1:], name[0])) # group B and I results\n",
        "\n",
        "    return y_pred, y_pred_mar, f1score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qhx6pJSHRrnW"
      },
      "source": [
        "## Model Input: \n",
        "* input features:\n",
        "    * word vector: pretrained traditional chinese word embedding by Word2Vec-CBOW\n",
        "    \n",
        "    (you may try add some other features, ex. pos-tag, word_length, word_position, ...) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "7UculQjKRrnW"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "tHtTSi_8RrnW"
      },
      "outputs": [],
      "source": [
        "# Load pretrained word vectors\n",
        "# Get a dict of tokens (key) and their pretrained word vectors (value)\n",
        "# Pre-trained word2vec CBOW word vector: https://fgc.stpi.narl.org.tw/activity/videoDetail/4b1141305ddf5522015de5479f4701b1\n",
        "# Pre-trained fastText word embeddings: https://fasttext.cc/docs/en/crawl-vectors.html\n",
        "# cc.zh.300.vec cna.cbow.cwe_p.tar_g.512d.0.txt\n",
        "dim = 0\n",
        "word_vecs= {}\n",
        "# Open pretrained word vector file\n",
        "with open(f'{folder}/cc.zh.300.vec') as f:\n",
        "    for line in f:\n",
        "        tokens = line.strip().split()\n",
        "\n",
        "        # there 2 integers in the first line: vocabulary_size, word_vector_dim\n",
        "        if len(tokens) == 2:\n",
        "            dim = int(tokens[1])\n",
        "            continue\n",
        "    \n",
        "        word = tokens[0] \n",
        "        vec = np.array([ float(t) for t in tokens[1:] ])\n",
        "        word_vecs[word] = vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ey0U_XhpRrnW",
        "outputId": "5b7b55d8-833d-455d-d97e-94793fc91c13",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vocabulary_size: 2000000\n",
            "word_vector_dim: (300,)\n"
          ]
        }
      ],
      "source": [
        "print(f'vocabulary_size: {len(word_vecs)}')\n",
        "print(f'word_vector_dim: {vec.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6RsMn2QRrnW"
      },
      "source": [
        "Here we split data into training dataset and testing dataset,\n",
        "however, we'll provide `development data` and `test data` which is real testing dataset.\n",
        "\n",
        "You should upload prediction on `development data` and `test data` to system, not this splitted testing dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "ySlER2ErRrnW"
      },
      "outputs": [],
      "source": [
        "# Load `train.data` and separate into a list of labeled data of each text\n",
        "# return:\n",
        "#   data_list: a list of lists of tuples, storing tokens and labels (wrapped in tuple) of each text in `train.data`\n",
        "#   traindata_list: a list of lists, storing training data_list splitted from data_list\n",
        "#   testdata_list: a list of lists, storing testing data_list splitted from data_list\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "def make_dataset(data_path):\n",
        "    with open(data_path, 'r', encoding='utf-8') as f:\n",
        "        data = f.readlines() #.encode('utf-8').decode('utf-8-sig')\n",
        "    data_list, data_list_tmp = list(), list()\n",
        "    article_id_list = list()\n",
        "    idx = 0\n",
        "    for row in data:\n",
        "        data_tuple = tuple()\n",
        "        if row == '\\n':\n",
        "            article_id_list.append(idx)\n",
        "            idx+=1\n",
        "            data_list.append(data_list_tmp)\n",
        "            data_list_tmp = []\n",
        "        else:\n",
        "            row = row.strip('\\n').split(' ')\n",
        "            data_tuple = (row[0], row[1])\n",
        "            data_list_tmp.append(data_tuple)\n",
        "    if len(data_list_tmp) != 0:\n",
        "        data_list.append(data_list_tmp)\n",
        "    \n",
        "    # Here we random split data into training dataset and testing dataset\n",
        "    # But you should take `development data` or `test data` as testing data\n",
        "    # At that time, you could just delete this line, \n",
        "    # nd generate data_list of `train data` and data_list of `development/test data` by this function\n",
        "    traindata_list, testdata_list, traindata_article_id_list, testdata_article_id_list=train_test_split(data_list,\n",
        "                                                                                                        article_id_list,\n",
        "                                                                                                        test_size=0.33,\n",
        "                                                                                                        random_state=42)\n",
        "    \n",
        "    return data_list, traindata_list, testdata_list, traindata_article_id_list, testdata_article_id_list "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "T3P8vbYGRrnX"
      },
      "outputs": [],
      "source": [
        "# look up word vectors\n",
        "# turn each word into its pretrained word vector\n",
        "# return a list of word vectors corresponding to each token in train.data\n",
        "\n",
        "\n",
        "def build_word_vectors(data_list, embedding_dict):\n",
        "    embedding_list = list()\n",
        "    temp = 0\n",
        "\n",
        "    # No Match Word (unknown word) Vector in Embedding\n",
        "    unk_vector = np.random.rand(*(list(embedding_dict.values())[0].shape))\n",
        "    # print(unk_vector)\n",
        "    for idx_list in range(len(data_list)):\n",
        "        embedding_list_tmp = list()\n",
        "        for idx_tuple in range(len(data_list[idx_list])):\n",
        "            key = data_list[idx_list][idx_tuple][0] # token\n",
        "            \n",
        "            \n",
        "            if key in embedding_dict:\n",
        "                value = embedding_dict[key]\n",
        "            else:\n",
        "                value = unk_vector\n",
        "            \n",
        "            embedding_list_tmp.append([key,value])\n",
        "        \n",
        "        embedding_list.append(embedding_list_tmp)\n",
        "    # print(embedding_list[0][0])\n",
        "    return embedding_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "qzmKa42yRrnX"
      },
      "outputs": [],
      "source": [
        "# Input features: pretrained word vectors of each token\n",
        "# Return a list of feature dicts, each feature dict corresponding to each token\n",
        "def make_features(embed_list, interval_list):\n",
        "    interval = 0\n",
        "    feature_list = list()\n",
        "    for idx_list in range(len(embed_list)):\n",
        "        feature_list_tmp = list()\n",
        "        for idx_tuple in range(len(embed_list[idx_list])):\n",
        "            feature_dict = dict()\n",
        "            for idx_vec in range(len(embed_list[idx_list][idx_tuple][1])):\n",
        "                feature_dict['dim_' + str(idx_vec+1)] = embed_list[idx_list][idx_tuple][1][idx_vec]\n",
        "            if ord('0') <= ord(embed_list[idx_list][idx_tuple][0]) <= ord('9'):\n",
        "                feature_dict['Is_num'] = True\n",
        "            else:\n",
        "                feature_dict['Is_num'] = False\n",
        "                if embed_list[idx_list][idx_tuple][0] == \".\" and feature_list_tmp[-1]['Is_num'] == True:\n",
        "                    feature_dict['Is_num'] = True\n",
        "            if embed_list[idx_list][idx_tuple][0] == \"！\" or embed_list[idx_list][idx_tuple][0] == \"？\" or embed_list[idx_list][idx_tuple][0] == \"：\" or embed_list[idx_list][idx_tuple][0] == \"。\" or embed_list[idx_list][idx_tuple][0] == \"，\" or  embed_list[idx_list][idx_tuple][0] == \"！\" :\n",
        "                feature_dict[\"Is_mark\"] = True\n",
        "            else:\n",
        "                feature_dict[\"Is_mark\"] = False\n",
        "\n",
        "            if embed_list[idx_list][idx_tuple][0][0] == \"B\" :\n",
        "                feature_dict[\"Interval\"] = interval_list[interval]\n",
        "                interval += 1\n",
        "            else:\n",
        "                feature_dict[\"Interval\"] = 0\n",
        "\n",
        "            feature_list_tmp.append(feature_dict)\n",
        "        feature_list.append(feature_list_tmp)\n",
        "\n",
        "    return feature_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "dNa4fSkYRrnX"
      },
      "outputs": [],
      "source": [
        "# Get the labels of each tokens in train.data\n",
        "# Return a list of lists of labels\n",
        "def process_labels(data_list):\n",
        "    label_list = list()\n",
        "    interval_list = list()\n",
        "    interval = 0\n",
        "    for idx_list in range(len(data_list)):\n",
        "        label_list_tmp = list()\n",
        "        for idx_tuple in range(len(data_list[idx_list])):\n",
        "            label_list_tmp.append(data_list[idx_list][idx_tuple][1])\n",
        "            if data_list[idx_list][idx_tuple][1][0] == \"B\" :\n",
        "                interval = 0\n",
        "            if data_list[idx_list][idx_tuple][1][0] == \"I\" and data_list[idx_list][idx_tuple + 1][1][0] != \"I\":\n",
        "                interval_list.append(interval + 1)\n",
        "            interval += 1\n",
        "        \n",
        "        label_list.append(label_list_tmp)\n",
        "        \n",
        "    return label_list, interval_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wdoz84b0RrnX"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "e_uGdFy3RrnX"
      },
      "outputs": [],
      "source": [
        "data_list, traindata_list, testdata_list, traindata_article_id_list, testdata_article_id_list = make_dataset(f\"{folder}/processed_data.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "BDc_mtuXRrnX"
      },
      "outputs": [],
      "source": [
        "# Load Word Embedding\n",
        "trainembed_list = build_word_vectors(traindata_list, word_vecs)\n",
        "testembed_list = build_word_vectors(testdata_list, word_vecs)\n",
        "\n",
        "# CRF - Train Data (Augmentation Data)\n",
        "y_train, interval_list = process_labels(traindata_list)\n",
        "x_train = make_features(trainembed_list, interval_list)\n",
        "\n",
        "\n",
        "# CRF - Test Data (Golden Standard)\n",
        "y_test, interval_list = process_labels(testdata_list)\n",
        "x_test = make_features(testembed_list, interval_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "hbhes9eBRrnX"
      },
      "outputs": [],
      "source": [
        "y_pred, y_pred_mar, f1score = CRF(x_train, y_train, x_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H02UitQBRrnX",
        "outputId": "c634760b-2f7c-486d-e81e-75aadce0981b",
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.5962773144411311"
            ]
          },
          "execution_count": 110,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "f1score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGwSu1AU0H1v"
      },
      "source": [
        "cc.zh.300.vec<br>\n",
        "with num     0.6012018963243285<br>\n",
        "without num  0.6023806685573265<br>\n",
        "add interval 0.5962773144411311\n",
        "\n",
        "another vectore <br>\n",
        "with num     0.35295256099491557<br>\n",
        "without num  0.36087741955476677<br>\n",
        "add interval 0.374521107776724<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0nOItHSRrnX"
      },
      "source": [
        "## Output data\n",
        "* Change model output into `output.tsv` \n",
        "* Only accept this output format uploading to competition system"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "QC66RB5tRrnX"
      },
      "outputs": [],
      "source": [
        "output=\"article_id\\tstart_position\\tend_position\\tentity_text\\tentity_type\\n\"\n",
        "for test_id in range(len(y_pred)):\n",
        "    pos=0\n",
        "    start_pos=None\n",
        "    end_pos=None\n",
        "    entity_text=None\n",
        "    entity_type=None\n",
        "    for pred_id in range(len(y_pred[test_id])):\n",
        "        if y_pred[test_id][pred_id][0]=='B':\n",
        "            start_pos=pos\n",
        "            entity_type=y_pred[test_id][pred_id][2:]\n",
        "        elif start_pos is not None and y_pred[test_id][pred_id][0]=='I' and y_pred[test_id][pred_id+1][0]=='O':\n",
        "            end_pos=pos\n",
        "            entity_text=''.join([testdata_list[test_id][position][0] for position in range(start_pos,end_pos+1)])\n",
        "            line=str(testdata_article_id_list[test_id])+'\\t'+str(start_pos)+'\\t'+str(end_pos+1)+'\\t'+entity_text+'\\t'+entity_type\n",
        "            output+=line+'\\n'\n",
        "        pos+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "40HnTWGtRrnX"
      },
      "outputs": [],
      "source": [
        "output_path=f'{folder}/output.tsv'\n",
        "with open(output_path,'w',encoding='utf-8') as f:\n",
        "    f.write(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-M36pQhxRrnX",
        "outputId": "44e0a673-b099-4f63-ee1e-fd549dc069b1",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "article_id\tstart_position\tend_position\tentity_text\tentity_type\n",
            "8\t10\t12\t38\tmed_exam\n",
            "8\t189\t193\t二十分鐘\ttime\n",
            "8\t293\t295\t五年\ttime\n",
            "8\t519\t521\t吩咐\ttime\n",
            "8\t540\t544\t兩個禮拜\ttime\n",
            "8\t858\t862\t前天下午\ttime\n",
            "8\t1354\t1356\t娜美\tname\n",
            "8\t1549\t1551\t五天\ttime\n",
            "8\t1622\t1627\t五天禮拜三\ttime\n",
            "8\t1939\t1941\t恍惚\ttime\n",
            "8\t1992\t1997\t禮拜三下午\ttime\n",
            "8\t2279\t2282\t185\tmed_exam\n",
            "8\t2377\t2380\t185\tmed_exam\n",
            "8\t2387\t2390\t185\tmed_exam\n",
            "8\t2560\t2563\t兩個月\ttime\n",
            "8\t2671\t2674\t155\tmed_exam\n",
            "8\t2679\t2682\t155\tmed_exam\n",
            "8\t2696\t2699\t155\tmed_exam\n",
            "16\t60\t66\t九、十點晚上\ttime\n",
            "16\t122\t124\t三年\ttime\n",
            "16\t130\t132\t三年\ttime\n",
            "16\t247\t249\t三年\ttime\n",
            "16\t592\t595\t5個月\ttime\n",
            "0\t55\t57\t68\tmed_exam\n",
            "0\t66\t68\t68\tmed_exam\n",
            "0\t435\t437\t歐洲\tlocation\n",
            "0\t1264\t1271\t10.78公分\tmed_exam\n",
            "0\t2523\t2526\t法馬上\ttime\n",
            "0\t2575\t2578\t四五天\ttime\n",
            "0\t2604\t2609\t3月18號\ttime\n",
            "0\t2630\t2635\t3月24日\ttime\n",
            "0\t2650\t2654\t3月24\ttime\n",
            "0\t2663\t2670\t禮拜二到禮拜四\ttime\n",
            "0\t2692\t2697\t3月31日\ttime\n",
            "24\t48\t51\t三個月\ttime\n",
            "24\t53\t56\t七公斤\tmed_exam\n",
            "24\t113\t115\t三年\ttime\n",
            "24\t141\t143\t三年\ttime\n",
            "24\t496\t499\t來箝制\ttime\n",
            "24\t536\t539\t來箝制\ttime\n",
            "24\t547\t550\t來箝制\ttime\n",
            "24\t1018\t1023\t5月28日\ttime\n",
            "24\t1196\t1200\t300塊\tmoney\n",
            "24\t1247\t1250\t30塊\tmoney\n",
            "24\t1398\t1401\t能馬上\ttime\n",
            "24\t1855\t1861\t九到十二個月\ttime\n",
            "24\t1866\t1872\t九到十二個月\ttime\n",
            "24\t1986\t1989\t五個月\ttime\n",
            "11\t18\t21\t九千七\tmoney\n",
            "11\t82\t86\t天九千七\tmoney\n",
            "11\t107\t110\t8.5\tmed_exam\n",
            "11\t111\t115\t12.4\tmed_exam\n",
            "11\t118\t121\t3.6\tmed_exam\n",
            "11\t135\t139\t三個禮拜\ttime\n",
            "11\t514\t518\t三個禮拜\ttime\n",
            "11\t557\t561\t三個禮拜\ttime\n",
            "11\t569\t573\t三個禮拜\ttime\n",
            "11\t645\t649\t三個禮拜\ttime\n",
            "11\t672\t676\t三個禮拜\ttime\n",
            "11\t693\t697\t兩個禮拜\ttime\n",
            "11\t708\t713\t四月二十日\ttime\n",
            "11\t751\t755\t兩個禮拜\ttime\n",
            "11\t1077\t1080\t666\tmed_exam\n",
            "11\t1084\t1087\t666\tmed_exam\n",
            "11\t1094\t1097\t098\tmed_exam\n",
            "11\t1101\t1104\t098\tmed_exam\n",
            "11\t1109\t1113\t7403\tmed_exam\n",
            "11\t1117\t1121\t7403\tmed_exam\n",
            "11\t1125\t1135\t6660897403\tmed_exam\n",
            "11\t1142\t1148\t666098\tmed_exam\n",
            "11\t1152\t1159\t0987403\tmed_exam\n",
            "9\t70\t74\t兩千多塊\tmoney\n",
            "9\t183\t190\t近85.5公斤\tmed_exam\n",
            "9\t273\t276\t泳四天\ttime\n",
            "9\t290\t294\t四十分鐘\ttime\n",
            "9\t743\t746\t374\tmed_exam\n",
            "9\t834\t837\t505\tmed_exam\n",
            "13\t37\t39\t95\tmed_exam\n",
            "13\t64\t66\t95\tmed_exam\n",
            "13\t74\t76\t95\tmed_exam\n",
            "13\t145\t150\t2月20號\ttime\n",
            "13\t153\t155\t78\tmed_exam\n",
            "13\t393\t397\t第六個月\ttime\n",
            "13\t435\t439\t500塊\tmoney\n",
            "13\t444\t448\t500塊\tmoney\n",
            "13\t549\t553\t第六個月\ttime\n",
            "13\t636\t641\t4月20號\ttime\n",
            "13\t662\t664\t5月\ttime\n",
            "13\t671\t673\t5月\ttime\n",
            "13\t678\t681\t5月初\ttime\n",
            "13\t701\t705\t兩個禮拜\ttime\n",
            "13\t721\t723\t半月\ttime\n",
            "13\t731\t735\t5月1號\ttime\n",
            "13\t774\t776\t5月\ttime\n",
            "13\t784\t788\t四個禮拜\ttime\n",
            "13\t793\t797\t四個禮拜\ttime\n",
            "13\t804\t808\t4月17\ttime\n",
            "13\t830\t834\t4月17\ttime\n",
            "13\t891\t896\t3月20號\ttime\n",
            "13\t946\t950\t四個禮拜\ttime\n",
            "13\t1050\t1055\t2月20號\ttime\n",
            "13\t1081\t1083\t37\tmed_exam\n",
            "1\t235\t237\t中午\ttime\n",
            "1\t254\t256\t中午\ttime\n",
            "1\t802\t805\t三個月\ttime\n",
            "1\t904\t908\t五十二百\tmoney\n",
            "1\t915\t919\t五十二百\tmoney\n",
            "1\t1052\t1055\t五個月\ttime\n",
            "1\t1340\t1344\t馬馬虎虎\tlocation\n",
            "1\t1349\t1353\t馬馬虎虎\tlocation\n",
            "1\t1382\t1387\t二零一六年\ttime\n",
            "1\t1394\t1396\t四年\ttime\n",
            "1\t1404\t1406\t四年\ttime\n",
            "1\t1414\t1416\t四年\ttime\n",
            "1\t2261\t2263\t半月\ttime\n",
            "1\t2295\t2299\t兩個禮拜\ttime\n",
            "1\t2334\t2338\t三個禮拜\ttime\n",
            "23\t275\t278\t三個月\ttime\n",
            "23\t339\t343\t4月27\ttime\n",
            "23\t535\t538\t三個月\ttime\n",
            "23\t540\t543\t二個月\ttime\n",
            "23\t556\t559\t三個月\ttime\n",
            "23\t629\t633\t8月1號\ttime\n",
            "23\t1584\t1588\t8月21\ttime\n",
            "23\t1592\t1596\t8月2號\ttime\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "kc8pEJ8aS9u_"
      },
      "outputs": [],
      "source": [
        "# # ## fail version\n",
        "\n",
        "# import pickle\n",
        "# # from plot_keras_history import plot_history\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.metrics import multilabel_confusion_matrix\n",
        "# from keras_contrib.utils import save_load_utils\n",
        "\n",
        "# from keras import layers\n",
        "# from keras import optimizers\n",
        "\n",
        "# from keras.models import Model\n",
        "\n",
        "\n",
        "# from tensorflow.python.keras.models import Input\n",
        "# # from keras.models import Input\n",
        "\n",
        "# from keras_contrib.layers import CRF\n",
        "# from keras_contrib import losses\n",
        "# from keras_contrib import metrics\n",
        "# # preprocessing\n",
        "# all_words = list(words)\n",
        "# all_tags = list(tags)\n",
        "# tag2index = {tag:idx + 1 for idx, tag in enumerate(all_tags)}\n",
        "# tag2index[\"--PADDING--\"] = 0\n",
        "# print(tags)\n",
        "# print(tag2index)\n",
        "# index2tag = {idx: word for word, idx in tag2index.items()}\n",
        "# print(index2tag)\n",
        "\n",
        "# word2index = {word: idx + 2 for idx, word in enumerate(all_words)}\n",
        "\n",
        "# word2index[\"--UNKNOWN_WORD--\"]=0\n",
        "\n",
        "# word2index[\"--PADDING--\"]=1\n",
        "\n",
        "# index2word = {idx: word for word, idx in word2index.items()}\n",
        "\n",
        "\n",
        "\n",
        "# MAX_SENTENCE = 0\n",
        "# WORD_COUNT = len(index2word)\n",
        "# DENSE_EMBEDDING = 50\n",
        "# LSTM_UNITS = 50\n",
        "# LSTM_DROPOUT = 0 # 0.1\n",
        "# DENSE_UNITS = 50 #100\n",
        "# BATCH_SIZE = 30 #256\n",
        "# MAX_EPOCHS = 5\n",
        "# TAG_COUNT = len(tag2index)\n",
        "# print(TAG_COUNT)\n",
        "\n",
        "\n",
        "# def MY_CRF(x_train, y_train, x_test, y_test):   ##(512dim, tags, 512dim, tags)\n",
        "#     # Doc: https://sklearn-crfsuite.readthedocs.io/en/latest/api.html#module-sklearn_crfsuite\n",
        "#     # crf = sklearn_crfsuite.CRF(\n",
        "#     #     algorithm='lbfgs',\n",
        "#     #     c1=0.1,\n",
        "#     #     c2=0.1,\n",
        "#     #     max_iterations=100,\n",
        "#     #     all_possible_transitions=True,\n",
        "#     # )\n",
        "#     # crf.fit(x_train, y_train)\n",
        "\n",
        "    \n",
        "\n",
        "#     # y_pred = crf.predict(x_test)\n",
        "#     # y_pred_mar = crf.predict_marginals(x_test)\n",
        "#     print(MAX_SENTENCE, MAX_SENTENCE * 300)\n",
        "\n",
        "#     input_layer = layers.Input(shape=(MAX_SENTENCE*300,))\n",
        "\n",
        "#     model = layers.Embedding(WORD_COUNT, DENSE_EMBEDDING, embeddings_initializer=\"uniform\", input_length=MAX_SENTENCE)(input_layer)\n",
        "\n",
        "#     model = layers.Bidirectional(layers.LSTM(LSTM_UNITS, recurrent_dropout=LSTM_DROPOUT, return_sequences=True))(model)\n",
        "\n",
        "#     model = layers.TimeDistributed(layers.Dense(DENSE_UNITS, activation=\"relu\"))(model)\n",
        "\n",
        "#     crf_layer = CRF(units=TAG_COUNT)\n",
        "#     output_layer = crf_layer(model)\n",
        "\n",
        "#     ner_model = Model(input_layer, output_layer)\n",
        "\n",
        "#     loss = losses.crf_loss\n",
        "#     acc_metric = metrics.crf_accuracy\n",
        "#     # acc_metric = metrics.accuracy_score\n",
        "#     opt = optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "#     ner_model.compile(optimizer=opt, loss=loss, metrics=[acc_metric])\n",
        "\n",
        "#     ner_model.summary()\n",
        "#     print(\"fit\")\n",
        "#     print(f\"x_train len = {len(x_train)}\")\n",
        "#     print(f\"x_train len[0] = {len(x_train[0])}\")\n",
        "#     print(f\"x_train len[0][0] = {len(x_train[0][0])}\")\n",
        "#     history = ner_model.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=MAX_EPOCHS, validation_split=0.1, verbose=2)\n",
        "\n",
        "#     print(\"fit finished\")\n",
        "\n",
        "        \n",
        "#     padded_sentence = x_test + [word2index[\"--PADDING--\"]] * (MAX_SENTENCE - len(x_test))\n",
        "#     padded_sentence = [word2index.get(w, 0) for w in padded_sentence]\n",
        "\n",
        "#     y_pred = ner_model.predict(np.array([padded_sentence]))\n",
        "#     # labels = list(crf.classes_)\n",
        "#     labels = list(tag2index)\n",
        "#     labels.remove('O')\n",
        "#     print(labels)\n",
        "#     f1score = metrics.flat_f1_score(y_test, y_pred, average='weighted', labels=labels)\n",
        "#     sorted_labels = sorted(labels,key=lambda name: (name[1:], name[0])) # group B and I results\n",
        "\n",
        "#     return y_pred,  f1score # y_pred_mar,\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36oJuO06RrnX"
      },
      "source": [
        "## Note\n",
        "* You may try `python-crfsuite` to train an neural network for NER tagging optimized by gradient descent back propagation\n",
        "    * [Documentation](https://github.com/scrapinghub/python-crfsuite)\n",
        "* You may try `CRF++` tool for NER tagging by CRF model\n",
        "    * [Documentation](http://taku910.github.io/crfpp/)\n",
        "    * Need design feature template\n",
        "    * Can only computed in CPU\n",
        "* You may try other traditional chinese word embedding (ex. fasttext, bert, ...) for input features\n",
        "* You may try add other features for NER model, ex. POS-tag, word_length, word_position, ...\n",
        "* You should upload the prediction output on `development data` or `test data` provided later to the competition system. Note don't upload prediction output on the splitted testing dataset like this baseline example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOuPLXj8RrnX"
      },
      "source": [
        "-----------------------------------------------------"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "file_extension": ".py",
    "kernelspec": {
      "display_name": "Python 3.7.3 64-bit ('base': conda)",
      "language": "python",
      "name": "python37364bitbaseconda25e194332beb4781976fd0b4d3e20954"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "mimetype": "text/x-python",
    "name": "python",
    "npconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": 3
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
